# ActiveMoE
The project aims to accelerate the inference speed of Mixture-of-Experts (MoE) large language models (LLM) on GPU memory-constrained devices, such as consumer-grade graphics cards and edge devices.
